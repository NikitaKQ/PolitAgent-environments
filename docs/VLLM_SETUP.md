# vLLM Benchmark Setup & Usage Guide

## üöÄ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
pip install -r requirements_vllm.txt

# –ò–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ pip
pip install vllm torch transformers huggingface_hub psutil GPUtil requests aiohttp pydantic numpy pandas
```

### 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π
mkdir -p models

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∞ HuggingFace (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –ø—Ä–∏–≤–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)
export HF_TOKEN="your_huggingface_token_here"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CUDA (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU)
export CUDA_VISIBLE_DEVICES=0
```

### 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ vLLM
python -c "import vllm; print('vLLM —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ')"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU (–µ—Å–ª–∏ –µ—Å—Ç—å)
python -c "import torch; print(f'CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}')"
```

## üìä –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞

### –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```bash
# –ó–∞–ø—É—Å–∫ —Å –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (–ª–µ–≥–∫–∏–µ –º–æ–¥–µ–ª–∏)
python scripts/vllm_benchmark_cli.py --preset example

# –ó–∞–ø—É—Å–∫ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (—Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)
python scripts/vllm_benchmark_cli.py --preset advanced
```

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏

```bash
# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ TinyLlama
python scripts/vllm_benchmark_cli.py --model "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º –∏–º–µ–Ω–µ–º
python scripts/vllm_benchmark_cli.py --model "microsoft/DialoGPT-medium" --name "–ú–æ–π DialoGPT"

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Qwen –º–æ–¥–µ–ª–∏
python scripts/vllm_benchmark_cli.py --model "Qwen/Qwen2-0.5B-Instruct"
```

### –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

```bash
# –ó–∞–ø—É—Å–∫ —Å DEBUG –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
python scripts/vllm_benchmark_cli.py --preset example --log-level DEBUG

# –£–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
python scripts/vllm_benchmark_cli.py --model "TinyLlama/TinyLlama-1.1B-Chat-v1.0" --results-dir custom_results
```

## üéØ –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

### Example Models (–ª–µ–≥–∫–∏–µ –º–æ–¥–µ–ª–∏)
- **TinyLlama 1.1B Chat** - –ú–∞–ª–µ–Ω—å–∫–∞—è —á–∞—Ç-–º–æ–¥–µ–ª—å
- **DialoGPT Medium** - –°—Ä–µ–¥–Ω—è—è –¥–∏–∞–ª–æ–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å
- **Qwen2 0.5B Instruct** - –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
- **DialoGPT Small** - –ú–∞–ª–µ–Ω—å–∫–∞—è –¥–∏–∞–ª–æ–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å

### Advanced Models (—Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)
- **DialoGPT Large** - –ë–æ–ª—å—à–∞—è –¥–∏–∞–ª–æ–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å
- **Qwen2 1.5B Instruct** - –°—Ä–µ–¥–Ω—è—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å  
- **Qwen2 7B Instruct** - –ë–æ–ª—å—à–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å

## üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏

–í—ã –º–æ–∂–µ—Ç–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Ä–µ–¥–∞–∫—Ç–∏—Ä—É—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ `scripts/vllm_benchmark_cli.py`:

```python
custom_model = {
    "model_path": "your-org/your-model",        # HuggingFace ID –º–æ–¥–µ–ª–∏
    "display_name": "–í–∞—à–∞ –ú–æ–¥–µ–ª—å",              # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è
    "tensor_parallel_size": 1,                  # –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –ø–æ —Ç–µ–Ω–∑–æ—Ä–∞–º
    "gpu_memory_utilization": 0.8,             # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏ (0.1-0.9)
    "max_model_len": 4096,                      # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    "port": 8000,                               # –ü–æ—Ä—Ç –¥–ª—è vLLM API
    "download_to_local": True,                  # –ó–∞–≥—Ä—É–∂–∞—Ç—å –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É
    "local_models_dir": "./models",             # –ü–∞–ø–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π
    "temperature": 0.7                          # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
}
```

## üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞ —Å–æ–∑–¥–∞–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:

```
vllm_benchmark_results/
‚îú‚îÄ‚îÄ Model_Name/
‚îÇ   ‚îú‚îÄ‚îÄ results_TIMESTAMP.json          # –ü–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
‚îÇ   ‚îî‚îÄ‚îÄ report_TIMESTAMP.md             # –ß–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π –æ—Ç—á–µ—Ç
‚îú‚îÄ‚îÄ vllm_benchmark_summary_TIMESTAMP.json  # –°–≤–æ–¥–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
‚îî‚îÄ‚îÄ vllm_benchmark.log                     # –õ–æ–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
```

## üí° –ü–æ–ª–µ–∑–Ω—ã–µ —Å–æ–≤–µ—Ç—ã

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤

1. **–î–ª—è –º–∞–ª–æ–º–æ—â–Ω—ã—Ö —Å–∏—Å—Ç–µ–º:**
   ```bash
   # –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏
   python scripts/vllm_benchmark_cli.py --model "microsoft/DialoGPT-small"
   ```

2. **–î–ª—è —Å–∏—Å—Ç–µ–º —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π GPU –ø–∞–º—è—Ç—å—é:**
   - –£–º–µ–Ω—å—à–∏—Ç–µ `gpu_memory_utilization` –¥–æ 0.4-0.6
   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º –¥–æ 1.5B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

3. **–î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:**
   - –ö–æ–¥ —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–≥—Ä –∏ —Ä–∞—É–Ω–¥–æ–≤
   - `num_games=1, max_rounds=3-6` –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏–≥—Ä—ã

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤

```bash
# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
watch -n 1 nvidia-smi

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
htop
```

### Troubleshooting

1. **–û—à–∏–±–∫–∞ CUDA out of memory:**
   - –£–º–µ–Ω—å—à–∏—Ç–µ `gpu_memory_utilization`
   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å
   - –£–º–µ–Ω—å—à–∏—Ç–µ `max_model_len`

2. **–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è:**
   - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
   - –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–∞ HuggingFace
   - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–æ–∫–µ–Ω HF_TOKEN –¥–ª—è –ø—Ä–∏–≤–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

3. **vLLM API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω:**
   - –î–æ–∂–¥–∏—Ç–µ—Å—å –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ (–¥–æ 5 –º–∏–Ω—É—Ç)
   - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –ø–æ—Ä—Ç –Ω–µ –∑–∞–Ω—è—Ç –¥—Ä—É–≥–∏–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º

## üéÆ –¢–µ—Å—Ç–∏—Ä—É–µ–º—ã–µ –∏–≥—Ä—ã

–ë–µ–Ω—á–º–∞—Ä–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö –∏–≥—Ä–∞—Ö:

1. **Diplomacy** - –î–∏–ø–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è (3 —Ä–∞—É–Ω–¥–∞)
2. **Beast** - –õ–æ–≥–∏—á–µ—Å–∫–∞—è –∏–≥—Ä–∞ (4 —Ä–∞—É–Ω–¥–∞) 
3. **Spyfall** - –î–µ—Ç–µ–∫—Ç–∏–≤–Ω–∞—è –∏–≥—Ä–∞ (4 —Ä–∞—É–Ω–¥–∞)
4. **AskGuess** - –ò–≥—Ä–∞ –≤ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã (6 —Ä–∞—É–Ω–¥–æ–≤)
5. **ToFuKingdom** - –†–æ–ª–µ–≤–∞—è –∏–≥—Ä–∞ (5 —Ä–∞—É–Ω–¥–æ–≤)

## üìà –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏

- **Overall Score** - –û–±—â–∏–π –±–∞–ª–ª –º–æ–¥–µ–ª–∏
- **Success Rate** - –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∏–≥—Ä
- **Average Response Time** - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
- **Memory Usage** - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU/RAM

## üîó –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [vLLM Documentation](https://docs.vllm.ai/)
- [HuggingFace Models Hub](https://huggingface.co/models)
- [Benchmark Runner Documentation](./scripts/benchmark_runner.py) 