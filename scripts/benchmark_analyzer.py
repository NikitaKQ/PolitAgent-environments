#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ PolitAgent.

–ü–æ–∑–≤–æ–ª—è–µ—Ç:
- –°—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤
- –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–¥–Ω—ã–µ –æ—Ç—á–µ—Ç—ã
- –°–æ–∑–¥–∞–≤–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Any, Optional
import argparse
from datetime import datetime

class BenchmarkAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤."""
    
    def __init__(self, results_dir: str = "benchmark_results"):
        self.results_dir = Path(results_dir)
        self.results_files = list(self.results_dir.glob("benchmark_results_*.json"))
        
    def load_results(self, file_path: Optional[str] = None) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞."""
        if file_path:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if not self.results_files:
            raise FileNotFoundError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤")
        
        latest_file = max(self.results_files, key=lambda f: f.stat().st_mtime)
        with open(latest_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def compare_models(self, results_data: Dict[str, Any]) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–µ—Ç DataFrame –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π."""
        ratings = results_data['model_ratings']
        
        df_data = []
        for rating in ratings:
            row = {
                'Model': rating['model_name'],
                'Overall Score': rating['overall_score'],
                'Success Rate': rating['success_rate'],
                'Avg Time (s)': rating['avg_execution_time'],
                'Total Games': rating['total_games'],
                'Rank': rating['rank']
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫–æ—Ä—ã –ø–æ –∏–≥—Ä–∞–º
            for game, score in rating['game_scores'].items():
                row[f'{game.title()} Score'] = score
            
            df_data.append(row)
        
        return pd.DataFrame(df_data)
    
    def generate_performance_plots(self, df: pd.DataFrame, output_dir: str = "plots"):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        # 1. –û–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π
        plt.figure(figsize=(12, 8))
        df_sorted = df.sort_values('Overall Score', ascending=True)
        plt.barh(df_sorted['Model'], df_sorted['Overall Score'])
        plt.title('–û–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π', fontsize=16, fontweight='bold')
        plt.xlabel('–û–±—â–∏–π –±–∞–ª–ª')
        plt.tight_layout()
        plt.savefig(output_path / 'overall_ranking.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Success Rate vs Execution Time
        plt.figure(figsize=(10, 8))
        scatter = plt.scatter(df['Success Rate'], df['Avg Time (s)'], 
                            s=df['Overall Score']*2, alpha=0.7)
        
        for i, model in enumerate(df['Model']):
            plt.annotate(model, (df.iloc[i]['Success Rate'], df.iloc[i]['Avg Time (s)']),
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        plt.xlabel('Success Rate')
        plt.ylabel('Average Execution Time (seconds)')
        plt.title('Success Rate vs Execution Time\n(—Ä–∞–∑–º–µ—Ä —Ç–æ—á–∫–∏ = –æ–±—â–∏–π –±–∞–ª–ª)')
        plt.tight_layout()
        plt.savefig(output_path / 'success_vs_time.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ –∏–≥—Ä–∞–º
        game_columns = [col for col in df.columns if col.endswith(' Score')]
        if game_columns:
            plt.figure(figsize=(14, 8))
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è heatmap
            heatmap_data = df.set_index('Model')[game_columns]
            
            sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='YlOrRd')
            plt.title('–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –ø–æ –∏–≥—Ä–∞–º', fontsize=16, fontweight='bold')
            plt.ylabel('–ú–æ–¥–µ–ª–∏')
            plt.xlabel('–ò–≥—Ä—ã')
            plt.tight_layout()
            plt.savefig(output_path / 'games_heatmap.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        print(f"üìä –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path}")
    
    def export_to_csv(self, df: pd.DataFrame, filename: str = None):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV."""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"benchmark_comparison_{timestamp}.csv"
        
        filepath = self.results_dir / filename
        df.to_csv(filepath, index=False)
        print(f"üìÑ CSV —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: {filepath}")
        return filepath
    
    def generate_summary_report(self, results_data: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç."""
        df = self.compare_models(results_data)
        
        report = f"""# –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –±–µ–Ω—á–º–∞—Ä–∫—É PolitAgent

## –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- **–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –º–æ–¥–µ–ª–µ–π**: {len(df)}
- **–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–≥—Ä**: {df['Total Games'].sum()}

## –¢–æ–ø-3 –º–æ–¥–µ–ª–∏ –ø–æ –æ–±—â–µ–º—É —Ä–µ–π—Ç–∏–Ω–≥—É:
"""
        
        top_3 = df.nlargest(3, 'Overall Score')
        for i, (_, row) in enumerate(top_3.iterrows()):
            report += f"{i+1}. **{row['Model']}** - {row['Overall Score']:.1f} –æ—á–∫–æ–≤\n"
        
        report += f"""
## –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:

### üèÜ –°–∞–º–∞—è –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
**{df.loc[df['Success Rate'].idxmax(), 'Model']}** - {df['Success Rate'].max():.1%} success rate

### ‚ö° –°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è
**{df.loc[df['Avg Time (s)'].idxmin(), 'Model']}** - {df['Avg Time (s)'].min():.1f} —Å–µ–∫—É–Ω–¥ –≤ —Å—Ä–µ–¥–Ω–µ–º

### üéØ –õ—É—á—à–∏–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
"""
        
        # –í—ã—á–∏—Å–ª—è–µ–º –±–∞–ª–∞–Ω—Å –∫–∞–∫ (success_rate / normalized_time)
        df_temp = df.copy()
        df_temp['normalized_time'] = df_temp['Avg Time (s)'] / df_temp['Avg Time (s)'].max()
        df_temp['balance_score'] = df_temp['Success Rate'] / (df_temp['normalized_time'] + 0.1)
        best_balance = df_temp.loc[df_temp['balance_score'].idxmax()]
        
        report += f"**{best_balance['Model']}** - –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏\n"
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –∏–≥—Ä–∞–º
        game_columns = [col for col in df.columns if col.endswith(' Score')]
        if game_columns:
            report += f"\n## –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –∏–≥—Ä–∞–º:\n"
            for game_col in game_columns:
                game_name = game_col.replace(' Score', '')
                best_model = df.loc[df[game_col].idxmax(), 'Model']
                best_score = df[game_col].max()
                report += f"- **{game_name}**: {best_model} ({best_score:.1f} –æ—á–∫–æ–≤)\n"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        report += f"""
## –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:

| –ú–µ—Ç—Ä–∏–∫–∞ | –°—Ä–µ–¥–Ω–µ–µ | –ú–µ–¥–∏–∞–Ω–∞ | –ú–∏–Ω | –ú–∞–∫—Å |
|---------|---------|---------|-----|------|
| –û–±—â–∏–π –±–∞–ª–ª | {df['Overall Score'].mean():.1f} | {df['Overall Score'].median():.1f} | {df['Overall Score'].min():.1f} | {df['Overall Score'].max():.1f} |
| Success Rate | {df['Success Rate'].mean():.1%} | {df['Success Rate'].median():.1%} | {df['Success Rate'].min():.1%} | {df['Success Rate'].max():.1%} |
| –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è | {df['Avg Time (s)'].mean():.1f}s | {df['Avg Time (s)'].median():.1f}s | {df['Avg Time (s)'].min():.1f}s | {df['Avg Time (s)'].max():.1f}s |

## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:

"""
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        best_overall = df.loc[df['Overall Score'].idxmax()]
        fastest = df.loc[df['Avg Time (s)'].idxmin()]
        most_accurate = df.loc[df['Success Rate'].idxmax()]
        
        if best_overall['Model'] == most_accurate['Model']:
            report += f"- **{best_overall['Model']}** –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ –ø–æ –æ–±—â–µ–º—É —Ä–µ–π—Ç–∏–Ω–≥—É, —Ç–∞–∫ –∏ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞.\n"
        else:
            report += f"- **{best_overall['Model']}** –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –æ–±—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.\n"
            report += f"- **{most_accurate['Model']}** —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è, –∫–æ–≥–¥–∞ –∫—Ä–∏—Ç–∏—á–Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å.\n"
        
        if fastest['Model'] != best_overall['Model']:
            report += f"- **{fastest['Model']}** –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ –≤–∞–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.\n"
        
        return report
    
    def compare_multiple_runs(self, file_paths: List[str]) -> pd.DataFrame:
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤."""
        all_data = []
        
        for file_path in file_paths:
            results = self.load_results(file_path)
            timestamp = results.get('timestamp', Path(file_path).stem)
            
            for rating in results['model_ratings']:
                row = {
                    'Run': timestamp,
                    'Model': rating['model_name'],
                    'Overall Score': rating['overall_score'],
                    'Success Rate': rating['success_rate'],
                    'Avg Time': rating['avg_execution_time']
                }
                all_data.append(row)
        
        return pd.DataFrame(all_data)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI."""
    parser = argparse.ArgumentParser(description="PolitAgent Benchmark Analyzer")
    parser.add_argument("--results-dir", "-d", default="benchmark_results",
                       help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏")
    parser.add_argument("--file", "-f", default=None,
                       help="–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    parser.add_argument("--export-csv", action="store_true",
                       help="–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ CSV")
    parser.add_argument("--generate-plots", action="store_true",
                       help="–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏")
    parser.add_argument("--output-dir", "-o", default="analysis_output",
                       help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞")
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer = BenchmarkAnalyzer(args.results_dir)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results_data = analyzer.load_results(args.file)
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞ –æ—Ç {results_data.get('timestamp', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
        
        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        df = analyzer.compare_models(results_data)
        print(f"üìà –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(df)} –º–æ–¥–µ–ª–µ–π")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≤—ã–≤–æ–¥–∞
        output_path = Path(args.output_dir)
        output_path.mkdir(exist_ok=True)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç
        summary_report = analyzer.generate_summary_report(results_data)
        report_file = output_path / "summary_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(summary_report)
        print(f"üìã –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
        
        # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ CSV
        if args.export_csv:
            csv_file = analyzer.export_to_csv(df, "model_comparison.csv")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
        if args.generate_plots:
            analyzer.generate_performance_plots(df, str(output_path / "plots"))
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É
        print("\nüèÜ –¢–û–ü-3 –ú–û–î–ï–õ–ò:")
        top_3 = df.nlargest(3, 'Overall Score')
        for i, (_, row) in enumerate(top_3.iterrows()):
            print(f"{i+1}. {row['Model']} - {row['Overall Score']:.1f} –æ—á–∫–æ–≤")
        
    except FileNotFoundError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã –∑–∞–ø—É—Å—Ç–∏–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ –∏ —Ñ–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—É—â–µ—Å—Ç–≤—É—é—Ç.")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")

if __name__ == "__main__":
    main() 