#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è vLLM –º–æ–¥–µ–ª—è–º–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º
—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø–∞–º—è—Ç—å—é GPU.
"""

import gc
import os
import psutil
import subprocess
import time
import logging
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import json
import asyncio
from dataclasses import dataclass
import GPUtil

logger = logging.getLogger(__name__)

@dataclass
class VLLMModelConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è vLLM."""
    model_path: str  # –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –∏–ª–∏ HuggingFace ID
    display_name: str
    tensor_parallel_size: int = 1
    gpu_memory_utilization: float = 0.8
    max_model_len: int = 4096
    temperature: float = 0.7
    trust_remote_code: bool = True
    port: int = 8000
    host: str = "127.0.0.1"

class VLLMResourceManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è vLLM."""
    
    def __init__(self, max_gpu_memory_mb: int = None):
        self.max_gpu_memory_mb = max_gpu_memory_mb
        self.current_process = None
        self.current_model = None
        
    def get_gpu_memory_usage(self) -> float:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU –≤ MB."""
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                return gpus[0].memoryUsed
            return 0.0
        except:
            return 0.0
    
    def get_system_memory_usage(self) -> float:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–π –ø–∞–º—è—Ç–∏ –≤ MB."""
        return psutil.virtual_memory().used / 1024 / 1024
    
    def check_memory_available(self, required_memory_mb: int) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –ø–∞–º—è—Ç–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏."""
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                available_gpu = gpus[0].memoryFree
                return available_gpu >= required_memory_mb
            return True  # –ï—Å–ª–∏ –Ω–µ—Ç GPU, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º CPU
        except:
            return True
    
    def cleanup_current_model(self):
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏."""
        if self.current_process:
            logger.info(f"–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å: {self.current_model}")
            try:
                self.current_process.terminate()
                self.current_process.wait(timeout=30)
            except subprocess.TimeoutExpired:
                self.current_process.kill()
                self.current_process.wait()
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            
            self.current_process = None
            self.current_model = None
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            gc.collect()
            time.sleep(5)  # –ñ–¥–µ–º –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
    
    def launch_vllm_model(self, config: VLLMModelConfig) -> bool:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç vLLM –º–æ–¥–µ–ª—å –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å."""
        try:
            # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –º–æ–¥–µ–ª—å
            self.cleanup_current_model()
            
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", config.model_path,
                "--host", config.host,
                "--port", str(config.port),
                "--tensor-parallel-size", str(config.tensor_parallel_size),
                "--gpu-memory-utilization", str(config.gpu_memory_utilization),
                "--max-model-len", str(config.max_model_len),
            ]
            
            if config.trust_remote_code:
                cmd.append("--trust-remote-code")
            
            logger.info(f"–ó–∞–ø—É—Å–∫–∞–µ–º vLLM –º–æ–¥–µ–ª—å: {config.display_name}")
            logger.info(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
            max_wait_time = 300  # 5 –º–∏–Ω—É—Ç
            start_time = time.time()
            
            while time.time() - start_time < max_wait_time:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –µ—â–µ –∂–∏–≤
                if process.poll() is not None:
                    stdout, stderr = process.communicate()
                    logger.error(f"vLLM –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π:")
                    logger.error(f"STDOUT: {stdout}")
                    logger.error(f"STDERR: {stderr}")
                    return False
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å API
                try:
                    import requests
                    response = requests.get(
                        f"http://{config.host}:{config.port}/health",
                        timeout=5
                    )
                    if response.status_code == 200:
                        logger.info(f"vLLM –º–æ–¥–µ–ª—å {config.display_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω–∞")
                        self.current_process = process
                        self.current_model = config.display_name
                        return True
                except:
                    pass
                
                time.sleep(10)
            
            # –¢–∞–π–º–∞—É—Ç
            logger.error(f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏ {config.display_name}")
            process.terminate()
            return False
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ vLLM –º–æ–¥–µ–ª–∏: {e}")
            return False

class SequentialVLLMBenchmark:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ vLLM –º–æ–¥–µ–ª–µ–π."""
    
    def __init__(self, results_dir: str = "vllm_benchmark_results"):
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        
        self.resource_manager = VLLMResourceManager()
        self.models_to_test = []
        self.all_results = []
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –±–µ–Ω—á–º–∞—Ä–∫-—Ä–∞–Ω–Ω–µ—Ä
        from scripts.benchmark_runner import BenchmarkRunner, ModelConfig
        self.BenchmarkRunner = BenchmarkRunner
        self.ModelConfig = ModelConfig
        
    def add_vllm_model(self, model_config: VLLMModelConfig):
        """–î–æ–±–∞–≤–ª—è–µ—Ç vLLM –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
        self.models_to_test.append(model_config)
    
    def add_models_from_config(self, models_config: List[Dict[str, Any]]):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        for config in models_config:
            vllm_config = VLLMModelConfig(**config)
            self.add_vllm_model(vllm_config)
    
    async def test_single_vllm_model(self, vllm_config: VLLMModelConfig) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–¥–Ω—É vLLM –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ—Ö –∏–≥—Ä–∞—Ö."""
        logger.info(f"üß™ –ù–∞—á–∏–Ω–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {vllm_config.display_name}")
        
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º vLLM –º–æ–¥–µ–ª—å
            if not self.resource_manager.launch_vllm_model(vllm_config):
                return {
                    "model_name": vllm_config.display_name,
                    "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å vLLM –º–æ–¥–µ–ª—å",
                    "status": "failed"
                }
            
            # –°–æ–∑–¥–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫-—Ä–∞–Ω–Ω–µ—Ä
            runner = self.BenchmarkRunner(str(self.results_dir / vllm_config.display_name.replace(" ", "_")))
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –∫–∞–∫ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—É—é —á–µ—Ä–µ–∑ vLLM endpoint
            runner.models = [self.ModelConfig(
                provider="openai",  # vLLM —Å–æ–≤–º–µ—Å—Ç–∏–º —Å OpenAI API
                model_name=vllm_config.model_path,
                display_name=vllm_config.display_name,
                temperature=vllm_config.temperature,
                api_key="EMPTY",  # vLLM –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∫–ª—é—á
                enabled=True
            )]
            
            # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—ã–π URL –¥–ª—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞
            import openai
            original_base_url = getattr(openai, 'base_url', None)
            openai.api_base = f"http://{vllm_config.host}:{vllm_config.port}/v1"
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏–≥—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            runner.configure_game("diplomacy", num_games=1, max_rounds=3)
            runner.configure_game("beast", num_games=1, max_rounds=4)
            runner.configure_game("spyfall", num_games=1, max_rounds=4)
            runner.configure_game("askguess", num_games=1, max_rounds=6)
            runner.configure_game("tofukingdom", num_games=1, max_rounds=5)
            
            logger.info(f"üéÆ –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è {vllm_config.display_name}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫
            start_time = time.time()
            benchmark_results = await runner.run_benchmark(max_parallel_models=1, max_parallel_games=1)
            total_time = time.time() - start_time
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–π—Ç–∏–Ω–≥–∏
            ratings = runner.calculate_model_ratings(benchmark_results)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results_file, report_file = runner.save_results(benchmark_results, ratings)
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –±–∞–∑–æ–≤—ã–π URL
            if original_base_url:
                openai.api_base = original_base_url
            
            result = {
                "model_name": vllm_config.display_name,
                "model_path": vllm_config.model_path,
                "status": "completed",
                "total_time": total_time,
                "results_file": str(results_file),
                "report_file": str(report_file),
                "ratings": [asdict(r) for r in ratings] if ratings else [],
                "benchmark_results": [asdict(r) for r in benchmark_results],
                "memory_usage": {
                    "gpu_memory_mb": self.resource_manager.get_gpu_memory_usage(),
                    "system_memory_mb": self.resource_manager.get_system_memory_usage()
                }
            }
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {vllm_config.display_name} –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∑–∞ {total_time:.1f}—Å")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ {vllm_config.display_name}: {e}")
            return {
                "model_name": vllm_config.display_name,
                "error": str(e),
                "status": "error"
            }
        
        finally:
            # –í—Å–µ–≥–¥–∞ –æ—á–∏—â–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            self.resource_manager.cleanup_current_model()
    
    async def run_sequential_benchmark(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π."""
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ vLLM –±–µ–Ω—á–º–∞—Ä–∫–∞")
        logger.info(f"üìä –ú–æ–¥–µ–ª–µ–π –∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é: {len(self.models_to_test)}")
        
        start_time = time.time()
        
        for i, vllm_config in enumerate(self.models_to_test, 1):
            logger.info(f"üìà –ú–æ–¥–µ–ª—å {i}/{len(self.models_to_test)}: {vllm_config.display_name}")
            
            result = await self.test_single_vllm_model(vllm_config)
            self.all_results.append(result)
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã
            if i < len(self.models_to_test):
                logger.info("‚è≥ –ü–∞—É–∑–∞ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏...")
                time.sleep(10)
        
        total_time = time.time() - start_time
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        summary = {
            "total_models": len(self.models_to_test),
            "completed_models": len([r for r in self.all_results if r.get("status") == "completed"]),
            "failed_models": len([r for r in self.all_results if r.get("status") in ["failed", "error"]]),
            "total_time": total_time,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "results": self.all_results
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        summary_file = self.results_dir / f"vllm_benchmark_summary_{int(time.time())}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"‚úÖ –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {total_time:.1f}—Å")
        logger.info(f"üìÑ –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {summary_file}")
        
        return summary
    
    def generate_comparison_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –≤—Å–µ–º –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –º–æ–¥–µ–ª—è–º."""
        if not self.all_results:
            return "–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"
        
        successful_results = [r for r in self.all_results if r.get("status") == "completed"]
        
        report = f"""# –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç vLLM –±–µ–Ω—á–º–∞—Ä–∫–∞

## –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –º–æ–¥–µ–ª–µ–π**: {len(self.all_results)}
- **–£—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ**: {len(successful_results)}
- **–ü—Ä–æ–≤–∞–ª–µ–Ω–æ**: {len(self.all_results) - len(successful_results)}

## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–æ–¥–µ–ª—è–º

"""
        
        for result in successful_results:
            if result.get("ratings"):
                rating = result["ratings"][0]  # –ü–µ—Ä–≤—ã–π (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π) —Ä–µ–π—Ç–∏–Ω–≥
                report += f"""### {result['model_name']}
- **–ü—É—Ç—å –º–æ–¥–µ–ª–∏**: `{result['model_path']}`
- **–û–±—â–∏–π –±–∞–ª–ª**: {rating.get('overall_score', 0):.1f}
- **Success Rate**: {rating.get('success_rate', 0):.1%}
- **–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è**: {result['total_time']:.1f}—Å
- **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ñ–∞–π–ª—ã**: {result['results_file']}

"""
        
        return report

# –ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
EXAMPLE_VLLM_MODELS = [
    {
        "model_path": "microsoft/DialoGPT-medium",
        "display_name": "DialoGPT Medium",
        "tensor_parallel_size": 1,
        "gpu_memory_utilization": 0.6,
        "max_model_len": 2048,
        "port": 8000
    },
    {
        "model_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "display_name": "TinyLlama 1.1B",
        "tensor_parallel_size": 1,
        "gpu_memory_utilization": 0.4,
        "max_model_len": 2048,
        "port": 8001
    },
    {
        "model_path": "microsoft/DialoGPT-small",
        "display_name": "DialoGPT Small",
        "tensor_parallel_size": 1,
        "gpu_memory_utilization": 0.3,
        "max_model_len": 1024,
        "port": 8002
    }
]

async def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è."""
    # –°–æ–∑–¥–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫
    benchmark = SequentialVLLMBenchmark()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª–∏ –∏–∑ –ø—Ä–∏–º–µ—Ä–∞
    benchmark.add_models_from_config(EXAMPLE_VLLM_MODELS)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫
    summary = await benchmark.run_sequential_benchmark()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    report = benchmark.generate_comparison_report()
    print(report)
    
    return summary

if __name__ == "__main__":
    asyncio.run(main())
