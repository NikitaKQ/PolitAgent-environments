name: "ollama"
provider: "ollama"

connection:
  base_url: "http://localhost:11434"
  timeout: 60

settings:
  temperature: 0.7
  max_tokens: 4096
  top_p: 0.9
  top_k: 40

available_models:
  - "gemma3:latest"
  - "llama3.1:latest"
  - "gemma:2b"
  - "gemma2:2b"

default_model: "gemma3:latest"

ollama_specific:
  stream: false
  format: "json"
  keep_alive: "5m" 