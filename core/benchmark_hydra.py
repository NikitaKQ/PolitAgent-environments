#!/usr/bin/env python
"""
PolitAgent Benchmark —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Hydra - —É–ª—É—á—à–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏.
"""

import logging
import os
import json
import multiprocessing
from typing import Dict, List, Any, Optional
from datetime import datetime
from pathlib import Path
import importlib
import random
import hydra
from omegaconf import DictConfig, OmegaConf
import traceback

from llm.models import get_model, get_available_models

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("benchmark_hydra")

# –ò–º–ø–æ—Ä—Ç –∏–≥—Ä–æ–≤—ã—Ö —Å—Ä–µ–¥ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ benchmark.py
from core.benchmark import GAME_ENVIRONMENTS, load_phrases, run_game

class HydraExperimentTracker:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å Hydra."""
    
    def __init__(self, cfg: DictConfig):
        self.cfg = cfg
        self.experiment_dir = Path(hydra.core.hydra_config.HydraConfig.get().runtime.output_dir)
        self.results: List[Dict[str, Any]] = []
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.results_dir = self.experiment_dir / "results"
        self.logs_dir = self.experiment_dir / "logs"
        self.artifacts_dir = self.experiment_dir / "artifacts"
        
        for dir_path in [self.results_dir, self.logs_dir, self.artifacts_dir]:
            dir_path.mkdir(exist_ok=True)
    
    def log_experiment_config(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞."""
        config_path = self.experiment_dir / "experiment_config.yaml"
        with open(config_path, 'w') as f:
            OmegaConf.save(self.cfg, f)
        logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {config_path}")
    
    def log_result(self, result: Dict[str, Any]):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ."""
        result["timestamp"] = datetime.now().isoformat()
        result["experiment_name"] = self.cfg.experiment.name
        self.results.append(result)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–∑—É –≤ JSONL —Ñ–∞–π–ª
        with open(self.results_dir / "all_results.jsonl", "a") as f:
            f.write(json.dumps(result) + "\n")
    
    def save_summary(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–≤–æ–¥–∫—É —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞."""
        summary = {
            "experiment": {
                "name": self.cfg.experiment.name,
                "description": self.cfg.experiment.description,
                "total_results": len(self.results),
                "start_time": datetime.now().isoformat(),
                "config": OmegaConf.to_container(self.cfg)
            },
            "results_summary": self._compute_summary_stats(),
            "games_performance": self._compute_game_performance()
        }
        
        summary_path = self.experiment_dir / "experiment_summary.json"
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        logger.info(f"–°–≤–æ–¥–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {summary_path}")
    
    def _compute_summary_stats(self) -> Dict[str, Any]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
        if not self.results:
            return {}
        
        successful = [r for r in self.results if "error" not in r]
        return {
            "total_runs": len(self.results),
            "successful_runs": len(successful),
            "success_rate": len(successful) / len(self.results) if self.results else 0,
            "games_tested": list(set(r.get("game_type") for r in self.results if r.get("game_type")))
        }
    
    def _compute_game_performance(self) -> Dict[str, Any]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ –∏–≥—Ä–∞–º."""
        game_stats = {}
        for result in self.results:
            game_type = result.get("game_type")
            if not game_type:
                continue
                
            if game_type not in game_stats:
                game_stats[game_type] = {
                    "total_runs": 0,
                    "successful_runs": 0,
                    "results": []
                }
            
            game_stats[game_type]["total_runs"] += 1
            if "error" not in result:
                game_stats[game_type]["successful_runs"] += 1
            game_stats[game_type]["results"].append(result)
        
        # –î–æ–±–∞–≤–ª—è–µ–º success rate –¥–ª—è –∫–∞–∂–¥–æ–π –∏–≥—Ä—ã
        for game_type, stats in game_stats.items():
            stats["success_rate"] = stats["successful_runs"] / stats["total_runs"] if stats["total_runs"] > 0 else 0
        
        return game_stats

def setup_model_from_config(cfg: DictConfig) -> Dict[str, Any]:
    """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Hydra."""
    model_config = cfg.model
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    model_kwargs = {
        "temperature": model_config.settings.temperature,
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
    if model_config.provider == "ollama":
        model_kwargs.update({
            "base_url": model_config.connection.base_url,
            "specific_model": model_config.default_model
        })
    elif model_config.provider == "openai":
        if "api" in model_config and "key" in model_config.api:
            model_kwargs["api_key"] = model_config.api.key
        model_kwargs["specific_model"] = model_config.default_model
    
    return get_model(model_config.provider, **model_kwargs)

def prepare_game_configs_from_hydra(cfg: DictConfig, tracker: HydraExperimentTracker) -> List[tuple]:
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–≥—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ Hydra config."""
    tasks = []
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏–≥—Ä –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    games_to_run = cfg.experiment.games if hasattr(cfg.experiment, 'games') else ["askguess"]
    
    for game_type in games_to_run:
        if game_type not in GAME_ENVIRONMENTS:
            logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∏–≥—Ä–∞: {game_type}")
            continue
        
        game_info = GAME_ENVIRONMENTS[game_type]
        
        # –°–æ–∑–¥–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –∏–≥—Ä—ã
        game_args = {}
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        game_args.update({
            "debug": cfg.experiment.get("debug", False),
            "workers": cfg.experiment.get("workers", 1),
            "runs_per_game": cfg.experiment.get("runs_per_game", 1),
            "max_phrases": cfg.experiment.get("max_phrases", None),
        })
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–≥—Ä—ã
        game_args.update(game_info["default_args"])
        
        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –º–æ–¥–µ–ª–∏
        for model_arg in game_info["model_args"]:
            game_args[model_arg] = cfg.model.provider
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        if cfg.model.provider == "ollama":
            game_args["specific_model"] = cfg.model.default_model
            game_args["ollama_base_url"] = cfg.model.connection.base_url
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ—Ä–∞–∑—ã –¥–ª—è –∏–≥—Ä, –∫–æ—Ç–æ—Ä—ã–º –æ–Ω–∏ –Ω—É–∂–Ω—ã
        if game_info["requires_phrases"]:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–≥—Ä—ã, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                if hasattr(cfg, 'game') and hasattr(cfg.game, 'settings') and hasattr(cfg.game.settings, 'label_path'):
                    game_args["label_path"] = cfg.game.settings.label_path
                
                phrases = load_phrases(game_type, type('Args', (), game_args)())
                if cfg.experiment.get("max_phrases"):
                    phrases = phrases[:cfg.experiment.max_phrases]
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ—Ä–∞–∑—ã –¥–ª—è {game_type}: {e}")
                phrases = [None]
        else:
            phrases = [None]
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ñ—Ä–∞–∑—ã –∏ –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
        for phrase in phrases:
            for run_id in range(cfg.experiment.runs_per_game):
                tasks.append((game_type, game_args, phrase, run_id, str(tracker.results_dir)))
    
    return tasks

@hydra.main(version_base="1.3", config_path="../configs", config_name="config")
def run_hydra_benchmark(cfg: DictConfig) -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞ —Å Hydra."""
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if cfg.output.log_level:
        logging.getLogger().setLevel(getattr(logging, cfg.output.log_level))
    
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ PolitAgent Benchmark —Å Hydra")
    logger.info(f"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {cfg.experiment.name}")
    logger.info(f"–û–ø–∏—Å–∞–Ω–∏–µ: {cfg.experiment.description}")
    logger.info(f"–ú–æ–¥–µ–ª—å: {cfg.model.provider} ({cfg.model.default_model})")
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–∫–µ—Ä —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    tracker = HydraExperimentTracker(cfg)
    tracker.log_experiment_config()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–¥–∞—á–∏
    tasks = prepare_game_configs_from_hydra(cfg, tracker)
    logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(tasks)} –∑–∞–¥–∞—á –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    if cfg.get("seed"):
        random.seed(cfg.seed)
        import numpy as np
        np.random.seed(cfg.seed)
    
    # –ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á
    if cfg.experiment.workers > 1:
        logger.info(f"–ó–∞–ø—É—Å–∫ –≤ –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–Ω–æ–º —Ä–µ–∂–∏–º–µ —Å {cfg.experiment.workers} –≤–æ—Ä–∫–µ—Ä–∞–º–∏")
        with multiprocessing.Pool(cfg.experiment.workers) as pool:
            results = pool.map(run_game, tasks)
    else:
        logger.info("–ó–∞–ø—É—Å–∫ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ")
        results = [run_game(task) for task in tasks]
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    for result in results:
        if result:
            tracker.log_result(result)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    tracker.save_summary()
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    summary_stats = tracker._compute_summary_stats()
    logger.info("=" * 60)
    logger.info("üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê")
    logger.info("=" * 60)
    logger.info(f"–í—Å–µ–≥–æ –∑–∞–ø—É—Å–∫–æ–≤: {summary_stats.get('total_runs', 0)}")
    logger.info(f"–£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤: {summary_stats.get('successful_runs', 0)}")
    logger.info(f"–£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {summary_stats.get('success_rate', 0):.2%}")
    logger.info(f"–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–≥—Ä—ã: {', '.join(summary_stats.get('games_tested', []))}")
    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {tracker.experiment_dir}")
    logger.info("=" * 60)

if __name__ == "__main__":
    run_hydra_benchmark() 